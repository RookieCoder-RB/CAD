{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the train set: 15195\n",
      "Number of images in the validation set: 3796\n",
      "Number of images in the test set: 6340\n",
      "Number of images in the train set(Hair removed): 15195\n",
      "Number of images in the validation set(Hair removed): 3796\n",
      "Number of images in the test set(Hair removed): 6340\n"
     ]
    }
   ],
   "source": [
    "# Checking len of documents\n",
    "\n",
    "nevus_path_test = 'test'\n",
    "nevus_path_train_0 = 'train/nevus'\n",
    "nevus_path_train_1 = 'train/others'\n",
    "nevus_path_val_0 = 'val/nevus'\n",
    "nevus_path_val_1 = 'val/others'\n",
    "\n",
    "nevus_train_0 = os.listdir(nevus_path_train_0)\n",
    "nevus_train_1 = os.listdir(nevus_path_train_1)\n",
    "total_train = nevus_train_0 + nevus_train_1\n",
    "print('Number of images in the train set:',len(total_train))\n",
    "\n",
    "nevus_val_0 = os.listdir(nevus_path_val_0)\n",
    "nevus_val_1 = os.listdir(nevus_path_val_1)\n",
    "total_val = nevus_val_0 + nevus_val_1\n",
    "print('Number of images in the validation set:',len(total_val))\n",
    "\n",
    "nevus_test = os.listdir(nevus_path_test)\n",
    "print('Number of images in the test set:',len(nevus_test))\n",
    "\n",
    "nevus_path_test = 'test'\n",
    "\n",
    "nevus_path_train_HairRemoved_0 = 'train_HairRemoved/nevus'\n",
    "nevus_path_train_HairRemoved_1 = 'train_HairRemoved/others'\n",
    "nevus_path_val_HairRemoved_0 = 'val_HairRemoved/nevus'\n",
    "nevus_path_val_HairRemoved_1 = 'val_HairRemoved/others'\n",
    "\n",
    "nevus_train_HairRemoved_0 = os.listdir(nevus_path_train_HairRemoved_0)\n",
    "nevus_train_HairRemoved_1 = os.listdir(nevus_path_train_HairRemoved_1)\n",
    "total_train_HairRemoved = nevus_train_HairRemoved_0 + nevus_train_HairRemoved_1\n",
    "print('Number of images in the train set(Hair removed):',len(total_train_HairRemoved))\n",
    "\n",
    "nevus_val_HairRemoved_0 = os.listdir(nevus_path_val_HairRemoved_0)\n",
    "nevus_val_HairRemoved_1 = os.listdir(nevus_path_val_HairRemoved_1)\n",
    "total_HairRemoved_val = nevus_val_HairRemoved_0 + nevus_val_HairRemoved_1\n",
    "print('Number of images in the validation set(Hair removed):',len(total_HairRemoved_val))\n",
    "\n",
    "nevus_test = os.listdir(nevus_path_test)\n",
    "print('Number of images in the test set(Hair removed):',len(nevus_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename columns for consistent naming\n",
    "def rename_columns(df, suffix):\n",
    "    return df.rename(columns=lambda x: f\"{x}_{suffix}\" if 'color_hist' in x or 'lbp' in x else x)\n",
    "\n",
    "# Load training datasets\n",
    "RGB_features = pd.read_csv('results/train/RGB_features_train.csv')\n",
    "HSV_features = pd.read_csv('results/train/HSV_features_train.csv')\n",
    "Lab_features = pd.read_csv('results/train/Lab_features_train.csv')\n",
    "lbp_features = pd.read_csv('results/train/lbp_features_train.csv')\n",
    "glcm_features = pd.read_csv('results/train/glcm_features_train.csv')\n",
    "\n",
    "# Apply consistent renaming\n",
    "RGB_features = rename_columns(RGB_features, \"rgb\")\n",
    "HSV_features = rename_columns(HSV_features, \"hsv\")\n",
    "Lab_features = rename_columns(Lab_features, \"lab\")\n",
    "lbp_features = rename_columns(lbp_features, \"lbp\")\n",
    "glcm_features = rename_columns(glcm_features, \"glcm\")\n",
    "\n",
    "# Merge features on 'image_path' and handle duplicate 'label' columns\n",
    "merged_data = RGB_features.merge(HSV_features, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_data = merged_data.drop(columns=[col for col in merged_data if col.endswith('_duplicate')])\n",
    "\n",
    "merged_data = merged_data.merge(Lab_features, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_data = merged_data.drop(columns=[col for col in merged_data if col.endswith('_duplicate')])\n",
    "\n",
    "merged_data = merged_data.merge(lbp_features, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_data = merged_data.drop(columns=[col for col in merged_data if col.endswith('_duplicate')])\n",
    "\n",
    "merged_data = merged_data.merge(glcm_features, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_data = merged_data.drop(columns=[col for col in merged_data if col.endswith('_duplicate')])\n",
    "\n",
    "\n",
    "# Ensure only one 'label' column remains and drop 'image_path' before training\n",
    "if 'label' in merged_data.columns:\n",
    "    label_column = merged_data.pop('label')\n",
    "    merged_data.insert(len(merged_data.columns), 'label', label_column)\n",
    "\n",
    "X_train = merged_data.drop(columns=['image_path', 'label'])  # Drop 'image_path' and 'label' for training features\n",
    "y_train = merged_data['label']\n",
    "\n",
    "# Load validation datasets\n",
    "RGB_features_val = pd.read_csv('results/val/RGB_features_val.csv')\n",
    "HSV_features_val = pd.read_csv('results/val/HSV_features_val.csv')\n",
    "Lab_features_val = pd.read_csv('results/val/Lab_features_val.csv')\n",
    "lbp_features_val = pd.read_csv('results/val/lbp_features_val.csv')\n",
    "glcm_features_val = pd.read_csv('results/val/glcm_features_val.csv')\n",
    "sift_features_val = pd.read_csv('results/val/sift_features_val.csv')\n",
    "\n",
    "# Apply consistent renaming\n",
    "RGB_features_val = rename_columns(RGB_features_val, \"rgb\")\n",
    "HSV_features_val = rename_columns(HSV_features_val, \"hsv\")\n",
    "Lab_features_val = rename_columns(Lab_features_val, \"lab\")\n",
    "lbp_features_val = rename_columns(lbp_features_val, \"lbp\")\n",
    "glcm_features_val = rename_columns(glcm_features_val, \"glcm\")\n",
    "\n",
    "# Merge validation features on 'image_path'\n",
    "merged_val_data = RGB_features_val.merge(HSV_features_val, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_val_data = merged_val_data.drop(columns=[col for col in merged_val_data if col.endswith('_duplicate')])\n",
    "\n",
    "merged_val_data = merged_val_data.merge(Lab_features_val, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_val_data = merged_val_data.drop(columns=[col for col in merged_val_data if col.endswith('_duplicate')])\n",
    "\n",
    "merged_val_data = merged_val_data.merge(lbp_features_val, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_val_data = merged_val_data.drop(columns=[col for col in merged_val_data if col.endswith('_duplicate')])\n",
    "\n",
    "merged_val_data = merged_val_data.merge(glcm_features_val, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_val_data = merged_val_data.drop(columns=[col for col in merged_val_data if col.endswith('_duplicate')])\n",
    "\n",
    "# Drop 'image_path' and separate features and labels for validation\n",
    "X_val = merged_val_data.drop(columns=['image_path', 'label'], errors='ignore')\n",
    "y_val = merged_val_data.get('label')  # Include 'label' if it exists\n",
    "\n",
    "# Repeat for test set, ensuring no 'label' column\n",
    "RGB_features_test = pd.read_csv('results/test/RGB_features_test.csv')\n",
    "HSV_features_test = pd.read_csv('results/test/HSV_features_test.csv')\n",
    "Lab_features_test = pd.read_csv('results/test/Lab_features_test.csv')\n",
    "lbp_features_test = pd.read_csv('results/test/lbp_features_test.csv')\n",
    "glcm_features_test = pd.read_csv('results/test/glcm_features_test.csv')\n",
    "\n",
    "# Apply consistent renaming\n",
    "RGB_features_test = rename_columns(RGB_features_test, \"rgb\")\n",
    "HSV_features_test = rename_columns(HSV_features_test, \"hsv\")\n",
    "Lab_features_test = rename_columns(Lab_features_test, \"lab\")\n",
    "lbp_features_test = rename_columns(lbp_features_test, \"lbp\")\n",
    "glcm_features_test = rename_columns(glcm_features_test, \"glcm\")\n",
    "\n",
    "# Merge test features on 'image_path' and drop 'image_path'\n",
    "merged_test_data = RGB_features_test.merge(HSV_features_test, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_test_data = merged_test_data.drop(columns=[col for col in merged_test_data if col.endswith('_duplicate')])\n",
    "\n",
    "merged_test_data = merged_test_data.merge(Lab_features_test, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_test_data = merged_test_data.drop(columns=[col for col in merged_test_data if col.endswith('_duplicate')])\n",
    "\n",
    "merged_test_data = merged_test_data.merge(lbp_features_test, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_test_data = merged_test_data.drop(columns=[col for col in merged_test_data if col.endswith('_duplicate')])\n",
    "\n",
    "merged_test_data = merged_test_data.merge(glcm_features_test, on='image_path', suffixes=('', '_duplicate'))\n",
    "merged_test_data = merged_test_data.drop(columns=[col for col in merged_test_data if col.endswith('_duplicate')])\n",
    "\n",
    "\n",
    "# Drop 'image_path' for test features\n",
    "X_test = merged_test_data.drop(columns=['image_path'])  # Test features only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    # 'RandomForest': {\n",
    "    #     'model': RandomForestClassifier(random_state=42),\n",
    "    #     'params': {\n",
    "    #         'n_estimators': [50, 100, 200],\n",
    "    #         'max_depth': [None, 10, 20],\n",
    "    #         'min_samples_split': [2, 5, 10]\n",
    "    #     }\n",
    "    # },\n",
    "    # 'SVM': {\n",
    "    #     'model': SVC(),\n",
    "    #     'params': {\n",
    "    #         'C': [0.1, 1, 10],\n",
    "    #         'kernel': ['linear', 'rbf'],\n",
    "    #         'gamma': ['scale', 'auto']\n",
    "    #     }\n",
    "    # },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [300],\n",
    "            'max_depth': [10, 12],\n",
    "            'learning_rate': [0.01, 0.1, 0.2]\n",
    "        }\n",
    "    },\n",
    "    # 'LogisticRegression': {\n",
    "    #     'model': LogisticRegression(solver='liblinear', random_state=42),\n",
    "    #     'params': {\n",
    "    #         'C': [0.01, 0.1, 1, 10, 100],      # Inverse of regularization strength\n",
    "    #         'penalty': ['l1', 'l2']             # Regularization norms\n",
    "    #     }\n",
    "    # },\n",
    "    # 'AdaBoost': {\n",
    "    #     'model': AdaBoostClassifier(random_state=42),\n",
    "    #     'params': {\n",
    "    #         'n_estimators': [50, 100, 200],     # Number of boosting rounds\n",
    "    #         'learning_rate': [0.01, 0.1, 1.0]   # Step size shrinkage\n",
    "    #     }\n",
    "    # },\n",
    "    # 'GradientBoosting': {\n",
    "    #     'model': GradientBoostingClassifier(random_state=42),\n",
    "    #     'params': {\n",
    "    #         'n_estimators': [50, 100, 200],\n",
    "    #         'max_depth': [3, 5, 10],\n",
    "    #         'learning_rate': [0.01, 0.1, 0.2]\n",
    "    #     }\n",
    "    # },\n",
    "    # 'KNN': {\n",
    "    #     'model': KNeighborsClassifier(),\n",
    "    #     'params': {\n",
    "    #         'n_neighbors': [3, 5, 7, 9, 11],     # Number of neighbors\n",
    "    #         'weights': ['uniform', 'distance'],  # Weight function\n",
    "    #         'metric': ['euclidean', 'manhattan'] # Distance metric\n",
    "    #     }\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame Shapes:\n",
      "RGB Features: (15195, 10)\n",
      "HSV Features: (15195, 26)\n",
      "Lab Features: (15195, 26)\n",
      "LBP Features: (15195, 20)\n",
      "GLCM Features: (15195, 10)\n"
     ]
    }
   ],
   "source": [
    "# Checking there is features\n",
    "\n",
    "print(\"Initial DataFrame Shapes:\")\n",
    "print(\"RGB Features:\", RGB_features.shape)\n",
    "print(\"HSV Features:\", HSV_features.shape)\n",
    "print(\"Lab Features:\", Lab_features.shape)\n",
    "print(\"LBP Features:\", lbp_features.shape)\n",
    "print(\"GLCM Features:\", glcm_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = np.concatenate([RGB_features.iloc[:, 1:].to_numpy(),HSV_features.iloc[:, 1:].to_numpy(),Lab_features.iloc[:, 1:].to_numpy(),lbp_features.iloc[:, 1:].to_numpy(), glcm_features.iloc[:, 1:].to_numpy()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging RGB + HSV: (15195, 34)\n",
      "After merging RGB + HSV + Lab: (15195, 58)\n",
      "After merging RGB + HSV + Lab + LBP: (15195, 76)\n",
      "After merging RGB + HSV + Lab + LBP + GLCM: (15195, 84)\n"
     ]
    }
   ],
   "source": [
    "# Check if all CSVs contain 'image_path' and 'label' columns\n",
    "required_columns = {'image_path', 'label'}\n",
    "for df_name, df in zip(['RGB','HSV','Lab' 'LBP', 'GLCM', 'SIFT'], \n",
    "                        [RGB_features,HSV_features,Lab_features, lbp_features, glcm_features]):\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        print(f\"Error: {df_name} features CSV is missing required columns {required_columns}\")\n",
    "        exit()\n",
    "        \n",
    "merged_data = RGB_features.merge(HSV_features, on=['image_path', 'label'], how='inner')\n",
    "print(\"After merging RGB + HSV:\", merged_data.shape)\n",
    "\n",
    "merged_data = merged_data.merge(Lab_features, on=['image_path', 'label'], how='inner')\n",
    "print(\"After merging RGB + HSV + Lab:\", merged_data.shape)\n",
    "\n",
    "merged_data = merged_data.merge(lbp_features, on=['image_path', 'label'], how='inner')\n",
    "print(\"After merging RGB + HSV + Lab + LBP:\", merged_data.shape)\n",
    "\n",
    "merged_data = merged_data.merge(glcm_features, on=['image_path', 'label'], how='inner')\n",
    "print(\"After merging RGB + HSV + Lab + LBP + GLCM:\", merged_data.shape)\n",
    "\n",
    "# Check if the merged data is empty\n",
    "if merged_data.empty:\n",
    "    print(\"Error: Merged training data is empty after merging all features. Check CSV files for mismatches in image_path values.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and tuning XGBoost...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 12, 'n_estimators': 300}\n",
      "Best cross-validation accuracy for XGBoost: 0.805923000987167\n",
      "\n",
      "Validation Results for XGBoost:\n",
      "Accuracy: 0.8250790305584826\n",
      "Kappa: 0.650148446619595\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83      1931\n",
      "           1       0.82      0.83      0.82      1865\n",
      "\n",
      "    accuracy                           0.83      3796\n",
      "   macro avg       0.83      0.83      0.83      3796\n",
      "weighted avg       0.83      0.83      0.83      3796\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1584  347]\n",
      " [ 317 1548]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each model and perform GridSearchCV\n",
    "best_estimators = {}\n",
    "for model_name, config in param_grids.items():\n",
    "    print(f\"Training and tuning {model_name}...\")\n",
    "    grid_search = GridSearchCV(config['model'], config['params'], cv=5, scoring='accuracy', n_jobs=-1, verbose=4)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_estimators[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy for {model_name}: {grid_search.best_score_}\")\n",
    "\n",
    "# Evaluate best models on the validation set\n",
    "for model_name, model in best_estimators.items():\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    kappa = cohen_kappa_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Results for {model_name}:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Kappa:\", kappa)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set predictions saved to results/test/predicted_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure X_test has the same columns as X_train\n",
    "missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0  # Add missing columns to X_test with zeros\n",
    "\n",
    "extra_cols = set(X_test.columns) - set(X_train.columns)\n",
    "X_test = X_test.drop(columns=extra_cols)  # Drop extra columns not in X_train\n",
    "\n",
    "# Reorder X_test columns to match X_train\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "best_model = best_estimators['XGBoost']\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Save predictions to CSV\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({\n",
    "    'predicted_label': y_test_pred\n",
    "})\n",
    "output_csv_path = 'results/test/predicted_labels.csv'\n",
    "output.to_csv(output_csv_path, index=False)\n",
    "print(f\"Test set predictions saved to {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
